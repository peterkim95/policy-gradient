{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bae055f-29c6-4bf8-a082-297a61bb1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "NOOP = 0\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "_ = env.reset()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.Grayscale(),\n",
    "                       T.Resize(80),\n",
    "                       T.ToTensor()])\n",
    "\n",
    "def preprocess(x):\n",
    "#     screen = env.render(mode='rgb_array') # (H,W,C) = (216,160,3)\n",
    "    x = x[35:195, :] # cut out the score and border\n",
    "    return transform(x)\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    '''\n",
    "    (N,1,80,80) -> (N,3) probability distribution of 3 actions\n",
    "    '''\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(3, 16, kernel_size=4, stride=2)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w, 8, 2), 4, 2)\n",
    "        convh = conv2d_size_out(conv2d_size_out(h, 8, 2), 4, 2)\n",
    "        linear_input_size = 16 * convw * convh\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "model = PolicyNetwork(80, 80, 3)\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "\n",
    "\n",
    "def select_action(x):\n",
    "    x = x.unsqueeze(0)\n",
    "    dist = Categorical(logits=model(x))\n",
    "    sampled_action = dist.sample()\n",
    "    log_p = dist.log_prob(sampled_action)\n",
    "    sampled_action = sampled_action.item()\n",
    "    if sampled_action == 0:\n",
    "        return NOOP, log_p\n",
    "    elif sampled_action == 1:\n",
    "        return UP_ACTION, log_p\n",
    "    elif sampled_action == 2:\n",
    "        return DOWN_ACTION, log_p\n",
    "    \n",
    "\n",
    "render = True\n",
    "batch_size = 10\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "def discount_rewards(r):\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        if r[t] != 0:\n",
    "            running_add = 0\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beaa5def-8cf5-4a9c-b485-9e091d94b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "rewards_received = []\n",
    "log_p_actions = []\n",
    "\n",
    "observation = env.reset()\n",
    "prev_x = preprocess(observation)\n",
    "while True:\n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    cur_x = preprocess(observation)\n",
    "    x = cur_x - prev_x\n",
    "    prev_x = cur_x\n",
    "    \n",
    "    action, log_p_action = select_action(x)\n",
    "    \n",
    "    log_p_actions.append(log_p_action)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    rewards_received.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        episode_number += 1\n",
    "        \n",
    "        discounted_rewards = discount_rewards(rewards_received)\n",
    "        \n",
    "        loss = (discounted_rewards * log_p_actions).sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        if episode_number % batch_size == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch = epsiode_number // batch_size\n",
    "            writer.add_scalar('Running Reward', running_reward, epoch)\n",
    "            \n",
    "        if running_reward is None:\n",
    "            running_reward = reward_sum\n",
    "        else:\n",
    "            running_reward = 0.99 * running_reward + 0.01 * reward_sum\n",
    "            \n",
    "        rewards_received, log_p_actions = [], []\n",
    "        reward_sum = 0\n",
    "        observation = env.reset()\n",
    "        prev_x = preprocess(observation)\n",
    "        \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "86845ab3-ccdb-4dd4-8ae3-a7941d6c3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.Grayscale(),\n",
    "                       T.Resize(80),\n",
    "                       T.ToTensor()])\n",
    "\n",
    "def preprocess(x):\n",
    "#     screen = env.render(mode='rgb_array') # (H,W,C) = (216,160,3)\n",
    "    x = x[35:195, :] # cut out the score and border\n",
    "    return transform(screen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a54c6b7-cb2d-48e3-885d-221a94a6e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    '''\n",
    "    (N,1,80,80) -> (N,3) probability distribution of 3 actions\n",
    "    '''\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(3, 16, kernel_size=4, stride=2)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w, 8, 2), 4, 2)\n",
    "        convh = conv2d_size_out(conv2d_size_out(h, 8, 2), 4, 2)\n",
    "        linear_input_size = 16 * convw * convh\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7640178e-f619-4943-84f5-af486ac78d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peter/miniconda3/envs/spinningup/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2312,  0.0720, -0.1642]], grad_fn=<AddmmBackward>),\n",
       " tensor([[0.3958, 0.3376, 0.2666]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PolicyNetwork(80, 80, 3)\n",
    "optimizer = optim.RMSprop(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68505cf-9ed1-413e-8fc5-77e0fcd3d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0631], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,a=select_action(torch.randn(1,80,80))\n",
    "_,b=select_action(torch.randn(1,80,80))\n",
    "([a,b] * np.array([12,51])).sum()\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "fcd8f72b-3649-4ddf-aa99-ac694af93e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(x):\n",
    "    x = x.unsqueeze(0)\n",
    "    dist = Categorical(logits=model(x))\n",
    "    sampled_action = dist.sample()\n",
    "    log_p = dist.log_prob(sampled_action)\n",
    "    sampled_action = sampled_action.item()\n",
    "    if sampled_action == 0:\n",
    "        return NOOP, log_p\n",
    "    elif sampled_action == 1:\n",
    "        return UP_ACTION, log_p\n",
    "    elif sampled_action == 2:\n",
    "        return DOWN_ACTION, log_p\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f78335b3-96e5-42f9-8386-e03a94da9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "render = True\n",
    "batch_size = 10\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9a6a13d7-d1ff-4221-8eb0-521d806f3d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 1094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "rewards_received = []\n",
    "log_p_actions = []\n",
    "\n",
    "observation = env.reset()\n",
    "prev_x = preprocess(observation)\n",
    "while True:\n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    cur_x = preprocess(observation)\n",
    "    x = cur_x - prev_x\n",
    "    prev_x = cur_x\n",
    "    \n",
    "    action, log_p_action = select_action(x)\n",
    "    \n",
    "    log_p_actions.append(log_p_action)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    rewards_received.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        episode_number += 1\n",
    "        \n",
    "        discounted_rewards = discount_rewards(rewards_received)\n",
    "        \n",
    "        loss = (discounted_rewards * log_p_actions).sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        if episode_number % batch_size == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch = epsiode_number // batch_size\n",
    "            writer.add_scalar('Running Reward', running_reward, epoch)\n",
    "            \n",
    "        if running_reward is None:\n",
    "            running_reward = reward_sum\n",
    "        else:\n",
    "            running_reward = 0.99 * running_reward + 0.01 * reward_sum\n",
    "            \n",
    "        rewards_received, log_p_actions = [], []\n",
    "        reward_sum = 0\n",
    "        observation = env.reset()\n",
    "        prev_x = preprocess(observation)\n",
    "        \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b1cbff8a-9025-43be-8f47-e2a9b8f05a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9801, 0.99  , 1.    ])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([0.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e9020fa6-7405-459e-8ee6-870e599cd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        if r[t] != 0:\n",
    "            running_add = 0\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fcf1a6e8-e27b-4397-b8fc-6b4f252fa580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frame list collector\n",
    "frames = []\n",
    "STEPS = 300\n",
    "\n",
    "# code for the two only actions in Pong\n",
    "NOOP = 0\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# initializing our environment\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "# beginning of an episode\n",
    "observation = env.reset()\n",
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dffdc0a2-b429-43e6-8ba9-323ea8b47db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcdd48ebb00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e73176e6-ff06-4161-94c6-1c7a9a7b3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "-1.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "-1.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "-1.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n",
      "0.0 False\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "env.reset()\n",
    "for i in range(STEPS):\n",
    "    # choose random action\n",
    "    env.render()\n",
    "    action = random.randint(UP_ACTION, DOWN_ACTION)\n",
    "\n",
    "    #run one step\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(observation) # collecting observation\n",
    "    print(reward, done)\n",
    "\n",
    "    # if episode is over, reset to beginning\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        frames.append(observation) # collecting observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c7e00d0-f1bf-42be-91ec-eea0103c2e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68cb7044-c485-49e5-9358-5e007c7e60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.Grayscale(),\n",
    "                       T.Resize(80),\n",
    "                       T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array') # (H,W,C) = (216,160,3)\n",
    "    screen = screen[35:195, :] # cut out the score and border\n",
    "    return transform(screen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8e0ee56-a3fa-4cc3-b22b-7dd12e1a611e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 80])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(screen).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4427102-cb4f-409c-9af8-1d1045b0a961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_height //2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9df117a0-bfb0-42d2-b78b-93650dc3efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e3a398-3310-48f2-8fad-df6ec04f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe8f362f-233f-47d0-acf9-b1fdbfdefb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a786a64-0cd6-4718-8be0-bed1b317ce80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([36., 81.]), tensor([-12.,  -8.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a536539-aa74-4426-a0a1-10f894084b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bf80f97-f734-43c5-ae16-03ce721b93ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3.], requires_grad=True), tensor([6., 4.], requires_grad=True))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "876c7767-e0a2-431a-9300-c22726001b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = a - lr * a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68fcadbb-d516-4744-9b73-2878be057b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = b - lr * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1adde2cd-c429-4e0b-8d44-e8b79c3d8bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-13.4170,  58.5505], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*ap**3 - bp**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65f94fd9-490f-4aa8-af1f-d59ede0cb880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.,  65.], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91537878-812c-483a-b77e-cee2768833e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6f3911566978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/spinningup/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spinningup/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2814d-5296-420a-94ae-bf490983d719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
